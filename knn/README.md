# ğŸ¤– KNN Mastery (K-Nearest Neighbors)

### ğŸ“Œ What is KNN?

A simple but powerful supervised machine learning algorithm that can be used for:

- âœ… Classification (e.g., Is this a cat or dog?)
- âœ… Regression (e.g., Predicting house price)

It predicts the label of a new data point by looking at the 'k' nearest data points in the training set and voting for the majority label.

------------


### ğŸ§  How KNN Thinks (Visual + Intuitive)
Imagine you're in a school cafeteria ğŸ”ğŸ¥¤ and you just transferred.

You donâ€™t know anyoneâ€¦
So to decide who you might be similar to, you look at the 3ï¸âƒ£ people (K=3) closest to you.
Theyâ€™re all talking about anime, so you think:
ğŸ‘‰ "I must be an anime fan too!"

Thatâ€™s how KNN works.

------------

### ğŸ“ Formula (KNN has no training formula â€” it's lazy!)

But for distance, it uses this:
ğŸ§® Euclidean Distance (most common):
```cpp
ğŸ§® Euclidean Distance (most common):
D = âˆš((x1 - x2)Â² + (y1 - y2)Â² + ... + (n1 - n2)Â²)
```
It just measures how close data points are.

------------

### âš™ï¸ How KNN Works Step-by-Step:

1. Save all the training data (no learning yet). 
2. When asked to make a prediction:
   - Measure distance between the test point and all training points.
   - Pick the K closest points (neighbors).
   - For classification: vote ğŸ—³ï¸
   - For regression: average ğŸ§®


------------

### ğŸ” Visual Intuition (Emoji Style)

```
ğŸ“¦ â† test point (what we want to predict)
ğŸ± ğŸ¶ ğŸ± ğŸ¶ ğŸ± â† labeled neighbors

If K=3:
â†’ Check 3 closest neighbors
â†’ Majority vote: ğŸ±ğŸ±ğŸ¶ â†’ ğŸ± wins â†’ prediction = ğŸ±

```


------------
### ğŸ§ª Sample Dataset (CSV-Ready)
| Height | Weight | Type    |
|--------|--------|---------|
| 160    | 55     | Burger  |
| 170    | 70     | Pizza   |
| 180    | 85     | Pizza   |
| 150    | 45     | Burger  |
| 165    | 65     | Pizza   |
| 155    | 50     | Burger  |
| 175    | 80     | Pizza   |
| 158    | 52     | Burger  |
| 172    | 77     | Pizza   |
| 153    | 49     | Burger  |


------------

### ğŸ§‘â€ğŸ’» Simple KNN Python Code (Pandas + Scikit-learn)

```python
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score

# Load Data
df = pd.read_csv("food_data.csv")  # replace with your CSV path

# Separate Features and Label
X = df[["Height", "Weight"]].values
y = df["Type"].values

# Encode Labels
le = LabelEncoder()
y = le.fit_transform(y)

# Split Dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# KNN Model
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Predictions
predictions = knn.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, predictions))
print(classification_report(y_test, predictions, target_names=le.classes_))

```

### ğŸ§  KNN Cheat Sheet

# ğŸ§  KNN Cheat Sheet

| **Concept**        | **Meaning**                                                            |
|--------------------|------------------------------------------------------------------------|
| `Lazy Learner`     | No training; only memorizes data                                        |
| `Distance Metric`  | Mostly Euclidean, can be Manhattan or others                           |
| `K Value`          | Number of neighbors to vote or average                                  |
| `Too Small K`      | Overfits (memorizes too much)                                           |
| `Too Big K`        | Underfits (too general)                                                |
| `Feature Scaling`  | Important, otherwise features like weight dominate                      |
| `Time Complexity`  | Slow on large data (O(n)) per prediction                               |


------------

#### âœ… Strengths (Pros)

-     Simple and intuitive

-     No training needed

-     Works well with small datasets

#### âŒ Weaknesses (Cons)

-     Slow with large datasets

-     Needs scaling (sensitive to feature range)

-     Bad with irrelevant features


------------

