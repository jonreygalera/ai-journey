# ğŸ§  Naive Bayes

### ğŸ“Œ What is Naive Bayes?

Imagine youâ€™re a burger detective. Youâ€™ve eaten hundreds of burgers ğŸ” and can guess what kind of burger it is (Spicy, Sweet, Cheesy) just by knowing whatâ€™s in itâ€”like sauce, toppings, etc.

Naive Bayes helps you guess the label (like burger type) based on ingredients (features). It uses math + probabilities to make the best guess.


------------

### ğŸ“š Used for:

- ğŸ“‚ Classification

- ğŸ§  Spam detection

- ğŸ¤– Sentiment analysis

- ğŸ©º Disease prediction

- ğŸ“Š Document categorization


------------

### ğŸ§® Formula (in plain English)
***Bayesâ€™ Theorem:***
P(Classâˆ£Data)=P(Dataâˆ£Class) * P(Class)P(Data)/P(Data)
*** But we ignore P(Data)P(Data) (same for all), so we use: ***
Score(Class)=P(Class)*P(Data1|Class)Ã—P(Data2|Class)*â€¦

------------


### ğŸ”‘ Naive Bayes Variables Cheat Sheet

| **Symbol / Term**          | **Meaning**                                                             |
|----------------------------|--------------------------------------------------------------------------|
| P(Class I Data)         | Probability the class is true, given the observed data (posterior)       |
| P(Data I Class)          | Probability of the data, assuming the class is true (likelihood)         |
| P(Class)                 | Prior probability of the class (how likely it is before seeing data)     |
| P(Data)                  | Probability of the data itself (evidence â€” often ignored in comparison)  |


------------

### ğŸ¯ Visual Intuition (Emoji style)

Letâ€™s say you see this:

ğŸ” â†’ [ğŸ”¥, ğŸ§€, ğŸŒ¶ï¸]
You're trying to decide if itâ€™s:

-     Spicy ğŸ”¥

-     Cheesy ğŸ§€

-     Sweet ğŸ¯

Naive Bayes counts how often each ingredient shows up for each class and uses that to guess the most likely class.


------------

### âš™ï¸ ASCII Logic Breakdown
```sql
+---------------------------+
|     Ingredient Count      |
+---------------------------+
| Class: Spicy              |
|  - ğŸ”¥: 30                 |
|  - ğŸ§€: 5                  |
+---------------------------+
| Class: Cheesy             |
|  - ğŸ”¥: 3                  |
|  - ğŸ§€: 40                 |
+---------------------------+

Now we predict:

Score(Spicy)  = P(Spicy) * P(ğŸ”¥|Spicy) * P(ğŸ§€|Spicy)  
Score(Cheesy) = P(Cheesy) * P(ğŸ”¥|Cheesy) * P(ğŸ§€|Cheesy)

â†’ Choose the class with the **highest score**
```


------------

#### âœ… Pros

  -   Very fast, even on large data

-     Works well with high-dimensional data

-     Easy to implement

#### âŒ Cons

-     Assumes features are independent (which is not always true)

-     Can perform poorly if features are correlated

### ğŸ“Š Metrics You Can Use

    Precision

    Recall

    F1-Score

    Accuracy
